Our work has identified several areas for improvement in the foundations of
existing bidirectional transformation tools. The development of symmetric
lenses enables both repositories associated with a lens to store locally
interesting information. Considering stateful, rather than pure,
transformations enables the lens to store this local information on the
side; though a theory of behavioral equivalence is needed to restore
equational reasoning in the presence of this state. This is an improvement
on the asymmetric lens framework's need for a canonical, centralized
repository, but the behavior of many of the actual lenses proposed in
Chapter~\ref{chap:symmetric} remain undesirable in several ways. Most
significantly, they employ a somewhat na\"ive strategy for divining the
connection between original and updated repositories. This manifests itself
as lenses which follow the letter of the law---produce synchronized
repositories---but mangle the meaning of the repositories during
synchronization by inappropriately mixing and matching data.

Edit lenses address this problem by elevating the status of the data
describing how original and updated repositories are connected. Treating
edits as first-class means that the lenses need not guess about alignment
information, and consequently will mangle the meaning of repositories less
often. Indeed, as a side effect, promoting edits in this way allows the
creation of stronger laws, so that the letter of the law and the spirit of
the law are no longer quite so far from each other. We have also shown that
one may realize performance benefits with this approach with careful design
of the edit languages being manipulated by edit lenses.

Related approaches to this problem are summarized in
Table~\ref{tab:related-commentary}; the edit lens framework is the first
approach to address all of the issues raised above. Asymmetric delta lenses
seem like a promising approach for their ability to handle alignment well,
but the formalism does not accomodate small edits well even if syntax could
be designed that could operate on them. Symmetric delta lenses extend these
to the symmetric setting, but the reasoning principles that require
behavioral equivalence discussed in conjunction with our symmetric lenses
are not considered, and the framework itself is not yet instantiated with a
syntax. The algebraic study of asymmetric lenses exposes many surprising
features of edits, but does not address many of the issues needed to create
a practical system. Matching lenses and annotation-based lenses are very
natural extensions of asymmetric, state-based lenses, but are also
conservative in many ways. The more radical changes proposed in edit lenses
allow for symmetric operation and eliminate the need to pass repositories to
the lenses. Notably, annotation-based lenses extend a variant of the
asymmetric, state-based lenses that allows for the construction of a lens
that duplicates information, making it the only approach that takes a
serious stab at alignment handling while enabling this lens. Finally,
constraint maintainers tackle many practical issues, but do not fully
explore the power of edits and lack the ability to perform sequential
composition, a key piece of syntax.

\begin{table}
    \begin{center}
        \begin{tabularx}{\linewidth}{m{4.4em}|XXXX}
        & Alignment & Symmetry & Performance & Syntax
        \\\hline
%        TGGs            &\N&\Y&\N&\Y&\\
        symm. state     &\N[very bad]
                        &\Y[yes; requires equivalence]
                        &\N[no]
                        &\Y[mostly domain agnostic]
        \\\hline
        asymm. $\delta$ &\Y[explicit alignments]
                        &\N[not a goal]
                        &\N[edits include repositories]
                        &\Y[via alternate framework]
        \\\hline
        symm. $\delta$  &\Y[edits]
                        &\Y[yes, but equiv. not explored]
                        &\N[edits include repositories]
                        &\N[alternate frameworks not instantiated]
        \\\hline
        algebraic       &\Y[edits]
                        &\N[no]
                        &\N[possibly, but unexplored]
                        &\N[not a goal]
        \\\hline
        matching        &\Y[mapping from holes to holes]
                        &\N[no]
                        &\N[repository and alignment information both
                        processed]
                        &\Y[variants of most AS-lens combinators]
        \\\hline
        annotated       &\Y[insertion, deletion, modification markers]
                        &\N[no]
                        &\N[alignment information includes repository]
                        &\Y[includes $\diag \in X \lens X \times X$]
        \\\hline
        const. maint.   &\Y[uninterpreted edits]
                        &\Y[yes; does not require equiv.]
                        &\N[no; all edits relative to $\init$]
                        &\Y[many primitives, but no composition]
    \end{tabularx}
    \end{center}
    \caption{Feature coverage for various alternatives to edit lenses}
    \label{tab:related-commentary}
\end{table}

Though the issues that edit lenses handle are important ones, there remain
many other foundational issues that one might want to tackle to strengthen
the practicality of bidirectional transformation frameworks. The following
section surveys a few of the most pressing needs.

\section{Future Work}
\label{sec:future}

\paragraph*{Hyperlenses}
% TODO
Some text copied here from the proposal document:

Finally, the lens framework focuses itself on the problem of synchronizing
two (potentially large) replicas at a time. The main new work proposed in
this document is to produce a generalization of lenses that can synchronize
very many (potentially quite small and loosely-related) replicas. For
concreteness, imagine a multi-directional spreadsheet: each cell is a
replica. Some cells are computed from others; these computations are the
transformations that we would like to bidirectionalize.

Lenses keep two similar pieces of data consistent; as either one evolves,
the lens finds analogous evolutions for the other. However, current lenses
don't generalize smoothly to more than two pieces of data. Spreadsheets
manage many pieces (cells) of data that are related to each other, but they
are generally unidirectional: some cells are special automatically-updated
cells, and the values in these cells are always computed by the system and
cannot be changed by the user. Constraint propagation systems generalize
spreadsheets to be many-directional when possible. However, current systems
do not use old states of the system to guide the computation of new states;
any system state which satisfies the given constraints is allowed.

The goal of the hyperlenses project is to merge the three systems, giving a
way of maintaining constraints between many pieces of data that, when given
an update to some part of the system, finds an ``analogous'' update to the
rest of the system. Below we discuss criteria on which the success of the
hyperlenses project can be judged.

An idea about how to wrap this up: we have several ideas (acyclic topology,
affine functions on reals only) about restricted environments that seem to
admit partial solutions, but none are really satisfactory. Lots of
connections to constraint propagation literature and (in the case of
functions on reals) computer algebra literature.

\paragraph*{Additional Syntax}
% TODO
\begin{itemize}
    \item more syntax
        \begin{itemize}
            \item edit modules for recursive types, fold/unfold lens combinators,
                investigate shape alignment
            \item edit modules for more container shapes, esp. graphs (for
                model-driven development) and relations (for databases)
        \end{itemize}
    \item wiring diagrams; some text copied here from the original
        complements paper:

        More speculatively, it is a well-known folklore result that
        symmetric monoidal categories are in 1-1 correspondence with wiring
        diagrams and with first-order linear lambda calculus. We would like
        to exploit this correspondence to design a lambda-calculus-like
        syntax for symmetric lenses and perhaps also a diagrammatic
        language. The linear lambda calculus has judgments of the form
        $x_1{:}A_1,\dots,x_n{:}A_n\vdash t:A_0$, where $A_0,\dots,A_n$ are
        sets or possibly syntactic type expressions and where $t$ is a
        linear term made up from basic lenses, lens combinators, and the
        variables $x_1,\dots,x_n$. This could be taken as denoting a
        symmetric lens $A_1\otimes\dots\otimes A_n\lens A_0$. For example,
        here is such a term for the lens $\mathit{concat}'$ from
        \S\ref{concatprime}:
        \[\begin{array}{@{}l}
        z{:}\Unit \oplus A \otimes A\LIST\otimes A\LIST\vdash
            \begin{array}[t]{@{}l}
            \textit{match}\ z\ \textit{with} \\
            \quad \mid\mlinl\unit \mapsto \const_{\NIL}\op\\
            \quad\mid\mlinr(a,al,ar)\mapsto \mathit{concat}(a\CONS al,ar)
            \end{array}
        \end{array}
        \]
        The interpretation of such a term in the category of lenses then
        takes care of the appropriate insertion of bijective lenses for
        regrouping and swapping tensor products.

        % TODO: possible citation: A survey of graphical languages for
        % monoidal categories, by Peter Selinger
\end{itemize}

\paragraph*{Algebraic Properties}
% TODO
\begin{itemize}
    \item associative sum
    \item symmetric monoidal structure of tensor product edit lens
    \item monoid laws for the various edit monoids
    \item partially-ordered homsets: perhaps we can't get a product or sum
        in our category, but could get a variant where the required
        equations are replaced by inequalities
\end{itemize}

\paragraph*{Implementation}
% TODO
The implementation could be used to explore a lot of stuff listed in the
planning document.

\paragraph*{Miscellaneous Extensions}
% TODO
\begin{enumerate}
    \item typed edits/complements; some text moved here from the related
        work section:

        One can view the two approaches as two extremes, with on one end
        graphs with a single node representing all possible repository
        states and on the other end graphs with many nodes where each node
        represents a single repository state. There may be a middle ground
        in which graph nodes each represent many possible repository states;
        the hope then would be that one could keep the benefit of a total
        edit application function while reusing single edits on many
        different states. For example, for list edits, one might consider a
        graph with one node for each possible length of list. Then one would
        have, for example, deletion edges $\mldelete : m \dedge n$ when
        $m<n$; such an edge must store marginally more information than our
        edit module did (the domain and codomain length rather than a single
        number telling their difference), but the set of repositories to
        which it applies is much more clearly delimited. Attempting to
        recast the edit modules and lenses proposed above in this light
        would be an interesting area for future work.
    \item parsing/unparsing string edits to structured edits
    \item automatically discover weight functions for symmetric lens
        iteration
    \item laws about undo-able edits, and requiring that \emph{edits} ``roundtrip''
    \item how closely do symmetric lens and edit lens constructions like
        tensor product and sum correspond? (can we get a theorem like the
        one saying how asymmetric lens constructions lift to symmetric lens
        constructions?)
    \item how closely do (symmetric) edit lenses and asymmetric delta lenses
        correspond? (can we get a theorem like the one saying how to
        factor a symmetric lens into two asymmetric ones?)
\end{enumerate}

\section{Closing Thoughts}
\label{sec:closing}
% TODO
Nevertheless lenses are part of an ecosystem of bidirectional techniques
aimed at the common task of maintaining replicated data and we expanded
what's known about incrementalizing and symmetrizing them, an important step
in the development of such techniques.
